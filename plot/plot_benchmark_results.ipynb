{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results_path = \"../results/\"\n",
    "data = []\n",
    "for root, dirs, files in os.walk(benchmark_results_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            df_ = pd.read_csv(os.path.join(root, file))\n",
    "            data.append(df_)\n",
    "\n",
    "df = pd.concat(data)\n",
    "human_and_random_scores = pd.read_csv(\"human_and_random_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_and_random_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Results for each Benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_reward'] = df.groupby(['model_name', 'env_id', 'prompt_chain', 'temperature', 'context_length'])['episode_reward'].transform('mean')\n",
    "df['std_reward'] = df.groupby(['model_name', 'env_id', 'prompt_chain', 'temperature', 'context_length'])['episode_reward'].transform('std')\n",
    "df['mean_length'] = df.groupby(['model_name', 'env_id', 'prompt_chain', 'temperature', 'context_length'])['episode_length'].transform('mean')\n",
    "df['std_length'] = df.groupby(['model_name', 'env_id', 'prompt_chain', 'temperature', 'context_length'])['episode_length'].transform('std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate data from random agent (baseline) and LLM agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"model_name\"] != \"random\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0. Precompute HNS ---------------------------------------------------\n",
    "\n",
    "baselines = human_and_random_scores.pivot(\n",
    "    index='env_id',\n",
    "    columns='model_name',\n",
    "    values='episode_reward'\n",
    ").rename(columns={'human':'R_human','random':'R_random'}).reset_index()\n",
    "\n",
    "df = df.merge(baselines, on='env_id', how='left')\n",
    "df['HNS'] = (df['mean_reward'] - df['R_random']) / (df['R_human'] - df['R_random'])\n",
    "\n",
    "# --- 1. Setup -------------------------------------------------------------\n",
    "\n",
    "# --- Setup ---\n",
    "palette         = dict(zip(df['model_name'].unique(), sns.color_palette(\"tab10\", df['model_name'].nunique())))\n",
    "prompt_chains   = sorted(df['prompt_chain'].unique())\n",
    "temperatures    = sorted(df['temperature'].unique())\n",
    "context_lengths = sorted(df['context_length'].unique())\n",
    "env_order       = sorted(df['env_id'].unique())\n",
    "\n",
    "df['env_id']       = pd.Categorical(df['env_id'], categories=env_order, ordered=True)\n",
    "df['prompt_chain'] = pd.Categorical(df['prompt_chain'], categories=prompt_chains, ordered=True)\n",
    "\n",
    "# --- Plot per-(env, context) ---\n",
    "for env in env_order:\n",
    "    for ctx in context_lengths:\n",
    "        sub = df[(df['env_id']==env) & (df['context_length']==ctx)]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        n_chains = len(prompt_chains)\n",
    "        fig, axes = plt.subplots(\n",
    "            n_chains, 1,\n",
    "            figsize=(6, 4 * n_chains),\n",
    "            sharey=True,\n",
    "            sharex=True\n",
    "        )\n",
    "        if n_chains == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for ax, prompt_chain in zip(axes, prompt_chains):\n",
    "            data = sub[sub['prompt_chain'] == prompt_chain]\n",
    "\n",
    "            data = data.drop_duplicates(\n",
    "                subset=['model_name', 'env_id', 'prompt_chain', 'temperature', 'context_length']\n",
    "            )\n",
    "\n",
    "            if data.empty:\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "\n",
    "            sns.barplot(\n",
    "                data=data,\n",
    "                x='temperature', y='HNS',\n",
    "                hue='model_name',\n",
    "                palette=palette,\n",
    "                ci=None,  # manual error bars\n",
    "                dodge=True,\n",
    "                width=0.8,\n",
    "                ax=ax\n",
    "            )\n",
    "\n",
    "            # Manual error bars (clean, index-aligned)\n",
    "            for i, bar in enumerate(ax.patches):\n",
    "                # Compute the center of the bar\n",
    "                center = bar.get_x() + bar.get_width() / 2\n",
    "                height = bar.get_height()\n",
    "\n",
    "                # Map bar to data row (i-th row in grouped barplot)\n",
    "                if i >= len(data):\n",
    "                    continue\n",
    "                row = data.iloc[i]\n",
    "\n",
    "                # Convert std to HNS units\n",
    "                std_norm = float(row['std_reward'] / (row['R_human'] - row['R_random']))\n",
    "\n",
    "                ax.errorbar(\n",
    "                    center,\n",
    "                    height,\n",
    "                    yerr=std_norm / np.sqrt(len(data)),\n",
    "                    fmt='none',\n",
    "                    ecolor='black',\n",
    "                    capsize=5,\n",
    "                    capthick=1,\n",
    "                    elinewidth=1,\n",
    "                    zorder=10\n",
    "                )\n",
    "\n",
    "            ax.set_ylabel(\"HNS\", fontsize=12)\n",
    "            ax.set_xlabel(\"Temperature\", fontsize=12)\n",
    "            ax.set_xticklabels(temperatures, rotation=45)\n",
    "            ax.set_title(f\"Prompt Chain: {prompt_chain}\", pad=10, fontsize=14)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "            # Remove per-plot legend\n",
    "            ax.legend_.remove()\n",
    "\n",
    "        # Shared legend\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        fig.legend(\n",
    "            handles, labels,\n",
    "            title=\"Model\",\n",
    "            loc='center',\n",
    "            bbox_to_anchor=(0.5, -0.15 / n_chains),\n",
    "            ncol=len(labels) // 2,\n",
    "            frameon=True,\n",
    "            fontsize=12,\n",
    "            title_fontsize=14,\n",
    "            borderaxespad=1\n",
    "        )\n",
    "\n",
    "        fig.suptitle(f\"{env} | Context Length: {ctx}\", fontsize=16, y=1.02, fontfamily='serif')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
